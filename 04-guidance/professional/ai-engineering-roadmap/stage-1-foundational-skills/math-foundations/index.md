# 1.6 Math Foundations (Optional)

Optional but helpful: Linear algebra, probability, statistics, and calculus basics.

---

## ðŸ“š What You'll Learn

- **Linear Algebra Basics**: Vectors and matrices, matrix operations, eigenvalues and eigenvectors
- **Probability & Statistics Basics**: Probability fundamentals, descriptive statistics, inferential statistics, causal thinking
- **Calculus Basics (Very Optional)**: Derivatives and gradients, optimization

**Note:** Not required for Stage 1, but helps with understanding AI models, ML algorithms, evaluating metrics properly, and optimization concepts. You can skip this and come back later.

---

## ðŸ”— Learning Resources

### Free Online
- **3Blue1Brown** (YouTube) - excellent visual explanations
  - Essence of Linear Algebra
  - Essence of Calculus
- **Khan Academy** - comprehensive coverage
  - Linear Algebra
  - Probability and Statistics
  - Calculus
- **StatQuest with Josh Starmer** - statistics focused

### Books
- "Essence of Algebra" (3Blue1Brown)
- "Introduction to Linear Algebra" (Strang)
- "Statistical Rethinking" (McElreath)

### Interactive
- **GeoGebra** - visualize math concepts
- **Desmos** - graphing calculator
- **NumPy/SciPy** - compute in Python

### Related Areas
- [1.5 Command Line & DevOps Basics](../command-line-devops/index.md)
- [1.7 Stage 1 Projects](../projects/index.md)
- [Stage 1 Overview](../index.md)

---

## Why Math?

Not required for Stage 1, but helps with:
- Understanding how AI models work (Stage 3)
- Grasping ML algorithms
- Evaluating metrics properly
- Optimization concepts

**You can skip this and come back later.**

---

## Linear Algebra Basics

Vectors and matrices are everywhere in ML.

### Vectors and Matrices

- [ ] **What is a vector?** (ordered list of numbers)
- [ ] **What is a matrix?** (2D grid of numbers)
- [ ] **Vector notation** (column vs row)
- [ ] **Matrix notation** and dimensions
- [ ] **Index notation** (accessing elements)

**Resources:**
- 3Blue1Brown Essence of Linear Algebra (YouTube)
- Khan Academy Linear Algebra

### Matrix Operations

- [ ] **Vector addition**
- [ ] **Scalar multiplication**
- [ ] **Dot product** (inner product)
- [ ] **Matrix multiplication**
- [ ] **Transpose** operation

**Resources:**
- NumPy documentation
- Linear Algebra basics

### Eigenvalues and Eigenvectors

- [ ] **Concept of eigenvectors**
- [ ] **Eigenvalues intuition**
- [ ] **Computing eigenvalues**
- [ ] **Applications in ML** (PCA, etc.)
- [ ] **Don't need to calculate by hand**

**Useful for:** Understanding embeddings, dimensionality reduction

---

## Probability & Statistics Basics

Essential for understanding model behavior.

### Probability Fundamentals

- [ ] **Probability basics** (0 to 1)
- [ ] **Conditional probability**
- [ ] **Independent events**
- [ ] **Bayes' theorem** (intuition, not formula)
- [ ] **Common distributions** (normal, exponential)

**Resources:**
- Khan Academy Probability
- StatQuest with Josh Starmer (YouTube)

### Descriptive Statistics

- [ ] **Mean, median, mode**
- [ ] **Standard deviation** (spread)
- [ ] **Variance**
- [ ] **Percentiles and quartiles**
- [ ] **Correlation** (relationship between variables)

**Practice:** Calculate stats on real datasets

### Inferential Statistics

- [ ] **Sampling** concepts
- [ ] **Confidence intervals**
- [ ] **Hypothesis testing** basics
- [ ] **P-values** intuition
- [ ] **Statistical significance**

**Useful for:** A/B testing, evaluation metrics

### Causal Thinking

- [ ] **Correlation vs causation**
- [ ] **Confounding variables**
- [ ] **Experimental design** basics
- [ ] **Observational vs experimental** studies
- [ ] **Simpson's paradox**

**Important for:** Avoiding misleading conclusions

---

## Calculus Basics (Very Optional)

Only if you want to understand optimization deeply.

### Derivatives and Gradients

- [ ] **Rate of change** (derivatives)
- [ ] **Partial derivatives** (multiple variables)
- [ ] **Gradient** (direction of steepest increase)
- [ ] **Chain rule** (for composite functions)
- [ ] **Intuition over calculation**

**Useful for:** Understanding backpropagation (Stage 3)

### Optimization

- [ ] **Finding maxima/minima**
- [ ] **Gradient descent** intuition
- [ ] **Learning rates** concept
- [ ] **Local vs global** minima
- [ ] **Why we use gradient descent** in ML

---

## âœ… Learning Checklist

### When to Study This

**Timeline:**
- **Stage 1**: Optional, focus on programming first
- **Stage 2**: Some helpful for understanding RAG/evaluation
- **Stage 3**: Important for deep learning (but can learn just-in-time)

**Recommendation:**
- Start with basics if interested
- Don't get bogged down in proofs
- Focus on intuition and application
- Learn more as needed in later stages

---

*Last updated: 2025-11-03*
